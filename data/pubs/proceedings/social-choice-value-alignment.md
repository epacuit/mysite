---
title: "Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
coauthor: 
  - Vincent Conitzer
  - Rachel Freedman
  - Jobst Heitzig
  - Wesley H. Holliday
  - Bob M. Jacobs
  - Nathan Lambert
  - Milan Mossé
  - Stuart Russell
  - Hailey Schoelkopf
  - Emanuel Tewolde
  - William S. Zwicker
authors: "Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, William S. Zwicker"
journal: 
year: 2024
type: proceedings
citation: ", presented at ICML 2024 and Social Choice and Welfare Conference 2024, Forthcoming."
volume:
number: 371
pages: 9346 - 9360
bookname: "ICML'24: Proceedings of the 41st International Conference on Machine Learning"
file: 
publisherlink:  https://dl.acm.org/doi/abs/10.5555/3692070.3692441
preprintlink:  https://arxiv.org/abs/2404.10271
additionaldata:
tags: 
  - Social Choice Theory
front_page: true
frontpage_data:
  - icon: link 
    short_blurb: "(with Vince Conitzer et al.), presented at  ICML 2024 and Social Choice and Welfare Conference 2024"
    use_publisher_link: false
    use_preprint_link: true
blurb: 
abstract: "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans' expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about 'collective' preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023."
---
    
